{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/malojan/nlp_nli/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[sentencepiece]==4.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.6\n",
      "  Downloading datasets-2.6.0-py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.5/441.5 kB\u001b[0m \u001b[31m868.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17 (from datasets==2.6)\n",
      "  Downloading numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m863.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0 (from datasets==2.6)\n",
      "  Downloading pyarrow-14.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.3.6 (from datasets==2.6)\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m794.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets==2.6)\n",
      "  Downloading pandas-2.1.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting requests>=2.19.0 (from datasets==2.6)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets==2.6)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets==2.6)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.6)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.11.1 (from fsspec[http]>=2021.11.1->datasets==2.6)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.6)\n",
      "  Downloading aiohttp-3.9.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0 (from datasets==2.6)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets==2.6) (23.2)\n",
      "Collecting responses<0.19 (from datasets==2.6)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets==2.6)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.6)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.6)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.6)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.6)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.6)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.6)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.6)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets==2.6)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets==2.6)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets==2.6)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets==2.6)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.6)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m253.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m975.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets==2.6) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.6)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets==2.6)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.6) (1.16.0)\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m172.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp311-cp311-macosx_11_0_arm64.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.8/386.8 kB\u001b[0m \u001b[31m818.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m880.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m389.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.2-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m505.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m660.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.1.4-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:16\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m202.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m514.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m492.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, pyyaml, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, yarl, requests, pyarrow, pandas, multiprocess, aiosignal, responses, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 attrs-23.1.0 certifi-2023.11.17 charset-normalizer-3.3.2 datasets-2.6.0 dill-0.3.5.1 filelock-3.13.1 frozenlist-1.4.1 fsspec-2023.12.2 huggingface-hub-0.20.1 idna-3.6 multidict-6.0.4 multiprocess-0.70.13 numpy-1.26.2 pandas-2.1.4 pyarrow-14.0.2 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 responses-0.18.0 tqdm-4.66.1 typing-extensions-4.9.0 tzdata-2023.3 urllib3-2.1.0 xxhash-3.4.1 yarl-1.9.4\n",
      "Collecting optuna==3.0\n",
      "  Downloading optuna-3.0.0-py3-none-any.whl (348 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.1/348.1 kB\u001b[0m \u001b[31m576.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting alembic (from optuna==3.0)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting cliff (from optuna==3.0)\n",
      "  Downloading cliff-4.4.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cmaes>=0.8.2 (from optuna==3.0)\n",
      "  Downloading cmaes-0.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting colorlog (from optuna==3.0)\n",
      "  Downloading colorlog-6.8.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from optuna==3.0) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from optuna==3.0) (23.2)\n",
      "INFO: pip is looking at multiple versions of optuna to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 1.11.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy<1.9.0,>=1.7.0 (from optuna) (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy<1.9.0,>=1.7.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]==4.23\n",
    "!pip install datasets==2.6\n",
    "!pip install optuna==3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/malo/Documents/interventions/nlp_nli/demo.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/malo/Documents/interventions/nlp_nli/demo.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## Load general packages\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/malo/Documents/interventions/nlp_nli/demo.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# some more specialised packages are loaded in each sub section\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/malo/Documents/interventions/nlp_nli/demo.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/malo/Documents/interventions/nlp_nli/demo.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "## Load general packages\n",
    "# some more specialised packages are loaded in each sub section\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "SEED_GLOBAL = 42\n",
    "np.random.seed(SEED_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twitter data\n",
    "\n",
    "df = pd.read_csv('twitter_sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training and test sets after sampling:  1000  (train)  8789  (test).\n"
     ]
    }
   ],
   "source": [
    "# Recode - 1 into 3\n",
    "df['sentiment'] = df['sentiment'].replace(-1,3)\n",
    "\n",
    "# Rename sentiment into label\n",
    "\n",
    "df = df.rename(columns={'sentiment': 'label'})\n",
    "\n",
    "# Create a label_text column \n",
    "\n",
    "df['label_text'] = df['label'].replace({0: 'Climate: neutral', 1: 'Climate: believe', 2: 'Climate: news', 3: 'Climate: deny'})\n",
    "\n",
    "# Split into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=SEED_GLOBAL)\n",
    "\n",
    "sample_size = 1000\n",
    "df_train = df_train.sample(n=min(sample_size, len(df_train)), random_state=SEED_GLOBAL).copy(deep=True)\n",
    "print(\"Length of training and test sets after sampling: \", len(df_train), \" (train) \", len(df_test), \" (test).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of each class in train set: \n",
      "Climate: believe    525\n",
      "Climate: news       222\n",
      "Climate: neutral    163\n",
      "Climate: deny        90\n",
      "Name: label_text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of each class in train set: \")\n",
    "print(df_train['label_text'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating hypothesis\n",
    "\n",
    "hypothesis_label_dic = {\n",
    "    \"Climate: news\" : \"(News): the tweet links to factual news about climate change\",\n",
    "    \"Climate: believe\": \"(Pro): the tweet supports the belief of man-made climate change\",\n",
    "    \"Climate: deny\": \"The tweet does not believe in man-made climate change\",\n",
    "    \"Climate: neutral\": \"Neutral: the tweet neither supports nor refutes the belief of man-made climate change\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df_train before formatting step: 1000.\n",
      "After adding not_entailment training examples, the training data was augmented to 1506 texts.\n",
      "Max augmentation could be: len(df_train) * 2 = 2000. It can also be lower, if there are more entail examples than not-entail for a majority class.\n"
     ]
    }
   ],
   "source": [
    "## function for reformatting the train set\n",
    "def format_nli_trainset(df_train=None, hypo_label_dic=None, random_seed=42):\n",
    "  print(f\"Length of df_train before formatting step: {len(df_train)}.\")\n",
    "  length_original_data_train = len(df_train)\n",
    "\n",
    "  df_train_lst = []\n",
    "  for label_text, hypothesis in hypo_label_dic.items():\n",
    "    ## entailment\n",
    "    df_train_step = df_train[df_train.label_text == label_text].copy(deep=True)\n",
    "    df_train_step[\"hypothesis\"] = [hypothesis] * len(df_train_step)\n",
    "    df_train_step[\"label\"] = [0] * len(df_train_step)\n",
    "    ## not_entailment\n",
    "    df_train_step_not_entail = df_train[df_train.label_text != label_text].copy(deep=True)\n",
    "    df_train_step_not_entail = df_train_step_not_entail.sample(n=min(len(df_train_step), len(df_train_step_not_entail)), random_state=random_seed)\n",
    "    df_train_step_not_entail[\"hypothesis\"] = [hypothesis] * len(df_train_step_not_entail)\n",
    "    df_train_step_not_entail[\"label\"] = [1] * len(df_train_step_not_entail)\n",
    "    # append\n",
    "    df_train_lst.append(pd.concat([df_train_step, df_train_step_not_entail]))\n",
    "  df_train = pd.concat(df_train_lst)\n",
    "  \n",
    "  # shuffle\n",
    "  df_train = df_train.sample(frac=1, random_state=random_seed)\n",
    "  df_train[\"label\"] = df_train.label.apply(int)\n",
    "  df_train[\"label_nli_explicit\"] = [\"True\" if label == 0 else \"Not-True\" for label in df_train[\"label\"]]  # adding this just to simplify readibility\n",
    "\n",
    "  print(f\"After adding not_entailment training examples, the training data was augmented to {len(df_train)} texts.\")\n",
    "  print(f\"Max augmentation could be: len(df_train) * 2 = {length_original_data_train*2}. It can also be lower, if there are more entail examples than not-entail for a majority class.\")\n",
    "\n",
    "  return df_train.copy(deep=True)\n",
    "\n",
    "\n",
    "df_train_formatted = format_nli_trainset(df_train=df_train, hypo_label_dic=hypothesis_label_dic, random_seed=SEED_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>label_text</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label_nli_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @RadioPakistan: Pakistan and Iran agree to ...</td>\n",
       "      <td>798478305146114048</td>\n",
       "      <td>Climate: news</td>\n",
       "      <td>(Pro): the tweet supports the belief of man-ma...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10770</th>\n",
       "      <td>1</td>\n",
       "      <td>World heat shatters records in 2016 in new sig...</td>\n",
       "      <td>817951192428986368</td>\n",
       "      <td>Climate: news</td>\n",
       "      <td>The tweet does not believe in man-made climate...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5257</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @SenSanders: We have a president-elect who ...</td>\n",
       "      <td>798338061662720000</td>\n",
       "      <td>Climate: believe</td>\n",
       "      <td>(Pro): the tweet supports the belief of man-ma...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31637</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @floodsg: Important response from @campaign...</td>\n",
       "      <td>959551491756445698</td>\n",
       "      <td>Climate: neutral</td>\n",
       "      <td>(Pro): the tweet supports the belief of man-ma...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24481</th>\n",
       "      <td>1</td>\n",
       "      <td>Scientists are getting better at linking extre...</td>\n",
       "      <td>887320023227920384</td>\n",
       "      <td>Climate: news</td>\n",
       "      <td>(Pro): the tweet supports the belief of man-ma...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27628</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NOAAResearch: A new study highlights the r...</td>\n",
       "      <td>915584592584826880</td>\n",
       "      <td>Climate: news</td>\n",
       "      <td>The tweet does not believe in man-made climate...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21202</th>\n",
       "      <td>0</td>\n",
       "      <td>I thought you'd like to know that pasta in mys...</td>\n",
       "      <td>865740085827862528</td>\n",
       "      <td>Climate: neutral</td>\n",
       "      <td>Neutral: the tweet neither supports nor refute...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>1</td>\n",
       "      <td>How a new money system could help stop climate...</td>\n",
       "      <td>794888626576429056</td>\n",
       "      <td>Climate: news</td>\n",
       "      <td>(Pro): the tweet supports the belief of man-ma...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8911</th>\n",
       "      <td>1</td>\n",
       "      <td>Reading: Guardian On climate change and the ec...</td>\n",
       "      <td>807694118646382592</td>\n",
       "      <td>Climate: believe</td>\n",
       "      <td>Neutral: the tweet neither supports nor refute...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33545</th>\n",
       "      <td>1</td>\n",
       "      <td>Mayors are taking the issues of affordable hou...</td>\n",
       "      <td>954058897622364160</td>\n",
       "      <td>Climate: news</td>\n",
       "      <td>The tweet does not believe in man-made climate...</td>\n",
       "      <td>Not-True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1506 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                            message  \\\n",
       "5407       1  RT @RadioPakistan: Pakistan and Iran agree to ...   \n",
       "10770      1  World heat shatters records in 2016 in new sig...   \n",
       "5257       0  RT @SenSanders: We have a president-elect who ...   \n",
       "31637      1  RT @floodsg: Important response from @campaign...   \n",
       "24481      1  Scientists are getting better at linking extre...   \n",
       "...      ...                                                ...   \n",
       "27628      1  RT @NOAAResearch: A new study highlights the r...   \n",
       "21202      0  I thought you'd like to know that pasta in mys...   \n",
       "1601       1  How a new money system could help stop climate...   \n",
       "8911       1  Reading: Guardian On climate change and the ec...   \n",
       "33545      1  Mayors are taking the issues of affordable hou...   \n",
       "\n",
       "                  tweetid        label_text  \\\n",
       "5407   798478305146114048     Climate: news   \n",
       "10770  817951192428986368     Climate: news   \n",
       "5257   798338061662720000  Climate: believe   \n",
       "31637  959551491756445698  Climate: neutral   \n",
       "24481  887320023227920384     Climate: news   \n",
       "...                   ...               ...   \n",
       "27628  915584592584826880     Climate: news   \n",
       "21202  865740085827862528  Climate: neutral   \n",
       "1601   794888626576429056     Climate: news   \n",
       "8911   807694118646382592  Climate: believe   \n",
       "33545  954058897622364160     Climate: news   \n",
       "\n",
       "                                              hypothesis label_nli_explicit  \n",
       "5407   (Pro): the tweet supports the belief of man-ma...           Not-True  \n",
       "10770  The tweet does not believe in man-made climate...           Not-True  \n",
       "5257   (Pro): the tweet supports the belief of man-ma...               True  \n",
       "31637  (Pro): the tweet supports the belief of man-ma...           Not-True  \n",
       "24481  (Pro): the tweet supports the belief of man-ma...           Not-True  \n",
       "...                                                  ...                ...  \n",
       "27628  The tweet does not believe in man-made climate...           Not-True  \n",
       "21202  Neutral: the tweet neither supports nor refute...               True  \n",
       "1601   (Pro): the tweet supports the belief of man-ma...           Not-True  \n",
       "8911   Neutral: the tweet neither supports nor refute...           Not-True  \n",
       "33545  The tweet does not believe in man-made climate...           Not-True  \n",
       "\n",
       "[1506 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for reformatting the test set\n",
    "def format_nli_testset(df_test=None, hypo_label_dic=None):\n",
    "  ## explode test dataset for N hypotheses\n",
    "  hypothesis_lst = [value for key, value in hypo_label_dic.items()]\n",
    "  print(\"Number of hypotheses/classes: \", len(hypothesis_lst))\n",
    "\n",
    "  # label lists with 0 at alphabetical position of their true hypo, 1 for not-true hypos\n",
    "  label_text_label_dic_explode = {}\n",
    "  for key, value in hypo_label_dic.items():\n",
    "    label_lst = [0 if value == hypo else 1 for hypo in hypothesis_lst]\n",
    "    label_text_label_dic_explode[key] = label_lst\n",
    "\n",
    "  df_test[\"label\"] = df_test.label_text.map(label_text_label_dic_explode)\n",
    "  df_test[\"hypothesis\"] = [hypothesis_lst] * len(df_test)\n",
    "  print(f\"Original test set size: {len(df_test)}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns must have matching element counts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s0/n85n2l8s46l90xmptvgpv2gh0000gn/T/ipykernel_62159/455869434.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# explode dataset to have K-1 additional rows with not_entail label and K-1 other hypotheses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ! after exploding, cannot sample anymore, because distorts the order to true label values, which needs to be preserved for evaluation code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hypothesis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multi-column explode requires pd.__version__ >= '1.3.0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test set size for NLI classification: {len(df_test)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/advanced_text_analysis_gesis_2023/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, column, ignore_index)\u001b[0m\n\u001b[1;32m   9034\u001b[0m             \u001b[0mmylen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9035\u001b[0m             \u001b[0mcounts0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmylen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9036\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9037\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmylen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9038\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"columns must have matching element counts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9039\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9040\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns must have matching element counts"
     ]
    }
   ],
   "source": [
    "  # explode dataset to have K-1 additional rows with not_entail label and K-1 other hypotheses\n",
    "  # ! after exploding, cannot sample anymore, because distorts the order to true label values, which needs to be preserved for evaluation code\n",
    "  df_test = df_test.explode([\"hypothesis\", \"label\"])  # multi-column explode requires pd.__version__ >= '1.3.0'\n",
    "  print(f\"Test set size for NLI classification: {len(df_test)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  df_test[\"label_nli_explicit\"] = [\"True\" if label == 0 else \"Not-True\" for label in df_test[\"label\"]]  # adding this just to simplify readibility\n",
    "\n",
    "  return df_test.copy(deep=True)\n",
    "\n",
    "\n",
    "df_test_formatted = format_nli_testset(df_test=df_test, hypo_label_dic=hypothesis_label_dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
