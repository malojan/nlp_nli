---
title: "Using Natural Language Inference for Text Classification"
author: "Malo Jan & Luis Sattelmayer"
format: revealjs
bibliography: references.bib
---

## Introduction {.smaller}

- Paper : @laurer_less_2023 
- Moritz Laurer : PhD in political science from Amsterdam, just started as Hugging Face ML engineer
- Recently released [zero-shot models](https://huggingface.co/models?pipeline_tag=zero-shot-classification) for text classification on HF
- Method paper on using Natural Language Inference for text classification in political science

## Context

- Trend in using Supervised Machine Learning in social sciences
- Important obstacles
  - Costs of annotation : human resources, time
  - Imbalanced data
- Annotation costs have been reduced by fine-tuning pre-trained models
- Increasing use of Generative models such as GPT for zero-shot/few-shot learning [@gilardi2023chatgpt ;@ollion2023chatgpt]

## Main contribution of the paper

- Distinction between two components of transfer Learning
- Language representations (Usually learned during pre-training with MLM) : eg. BERT
- Task representations : 
  - Usually learned during fine-tuning because BERT trained on MLM task not on a classification task
- Use of NLI to transfer both language and task representations for text classification

## What is NLI ? {.smaller}

- **N**atural **L**anguage **I**nference (also called Recognizing Textual Entailment; RTE)
- It consists of two components: 
  1. Context (Premise)
  2. Hypothesis

- The task is to determine whether the hypothesis is true or false given the premise
- In NLI terms: 
  - hypothesis corresponds with the premise: *entailment*
  - hypothesis is the opposite of the premise: *contradiction*
  - hypothesis is neither the same nor the opposite of the premise: *neutral*
  
## Simple NLI example {.smaller}

:::{.fragment .nonincremental}
- Context: *The cat is on the mat*
- Hypothesis: *The cat is on sitting in the garden*
- Label: **contradiction**
:::

:::{.fragment .nonincremental}
- Context: *The cat is on the mat*
- Hypothesis: *The cat is sitting on a piece of furniture*

- Label: **entailment**
:::

:::{.fragment .nonincremental}
- Context: *The cat is on the mat*
- Hypothesis: *I love NLI and I want to try it out immediately*

- Label: **neutral**
:::


## NLI as a classification task {.smaller}

- @yin2020universal, @wang2021entailment
- See Yin, Hay and Roth 2019 and refined Wang et al 2021
-   Classic Bert fine-tuning for text classification
    -   Transfer of the language knowledge
    -   Pre-trained model has no knowledge of the task nor the specific domain
-   BERT-NLI
    -   Transfer of the language & task knowledge
    -   NLI is a task that can be applied to any classification task
    -   Works well because data rich task

- Model : Deberta fine-tuned on 8 general purpose NLI datasets on more than 1 million examples of hypothesis context pairs

## Methods and design {.smaller}

-   Data : several manually annotated datasets in political science (CMP, CAP, CoronatNet...) of different sizes, domain, unit
-   8 classification tasks
-   Comparison of the following models at different ammount of training data [0,100,500,1000,2500,5000,1000] : 

| Model | SVM tf-idf | LR tf-idf | SVM-embeddings | LR-embeddings | Deberta-base | BERT-NLI |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| knowledge transfer |No |No |Shallow |Shallow | Deep | Deep |
| task transfer |No |No |No |No |No | Deep  |

## 

![](results.png){width="90%" height="70%" fig-align="center"}

## Conclusion : pros of NLI for text classification {.smaller}

- Useful when : 
  - Little training data
  - Imbalanced data : BERT-NLI can predict classes without having seen a single exemple (zero shot)



-  Less useful when large data, BERT fine-tuning similar or better : 
   -  After 2000 exemples : convergence of performance and then BERT outperforms BERT-NLI

- Other limits : 
  - Perform less well with complex concepts ( traditional morality) than simpler (military positive) 
  - NLI english data rich task but not necessarily in other languages even if multilingual models are available


## NLI & ChatGpt for zero-shot/few shot learning

- NLI for text classification can be used for zero-shot/few shot learning
- In constrast to ChatGpt :
  -  Free/open source
  -  Less computationally intensive

- Allows label verbalization (similar as a prompt) : 
  - Pros : Task can be express in plain language
  - Cons : results depend on how the hypothesis is formulated (kind of a hyperparameter), same issues than In context learning

-   Does this really solve the annotation issue ?
    -   Use of zero-shot NLI as initial model to have active learning exemples in a first annotation round ? 


## References

-   [Github repo](https://github.com/MoritzLaurer/less-annotating-with-bert-nli/tree/master)
